{
    "n_positions": 1024,
    "n_embed": 2048,
    "n_layers": 20,
    "n_heads": 8,
    "n_ff": 8192,
    "n_vocab": 32768,
    "act_fn": "DecoderLayer(\n  (mh_attn): MultiHeadAttention(\n    (layer_1): Linear(in_features=2048, out_features=6144, bias=True)\n    (W_o): Linear(in_features=2048, out_features=2048, bias=True)\n  )\n  (mha_dropout): Dropout(p=0.2, inplace=False)\n  (mha_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n  (ff_layers): Sequential(\n    (0): Linear(in_features=2048, out_features=8192, bias=True)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=8192, out_features=2048, bias=True)\n  )\n  (ff_dropout): Dropout(p=0.2, inplace=False)\n  (ff_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n)",
    "dropout": "unknown",
    "stats": {
        "iter_through": 1500,
        "tok_through": 3072000,
        "losses": [
            11.625,
            11.5,
            11.4375,
            11.5,
            11.4375,
            11.1875,
            11.3125,
            11.5,
            11.375,
            11.4375,
            11.375,
            11.4375,
            11.375,
            11.4375,
            11.3125,
            11.4375,
            11.5625,
            11.4375,
            11.6875,
            11.375,
            11.125,
            11.3125,
            11.375,
            11.3125,
            11.25,
            11.0625,
            11.3125,
            11.3125,
            10.875,
            11.375
        ],
        "tok_snap": 217088,
        "time_snap": 1753810755.012453,
        "run_time_start": 1753810035.515195,
        "run_tok_through": 3072000,
        "run_iter_through": 1500
    }
}