{
    "n_positions": 1024,
    "n_embed": 1536,
    "n_layers": 16,
    "n_heads": 6,
    "n_ff": 6144,
    "n_vocab": 32760,
    "act_fn": "DecoderLayer(\n  (mh_attn): MultiHeadAttention(\n    (layer_1): Linear(in_features=1536, out_features=4608, bias=True)\n    (W_o): Linear(in_features=1536, out_features=1536, bias=True)\n  )\n  (mha_dropout): Dropout(p=0.2, inplace=False)\n  (mha_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n  (ff_layers): Sequential(\n    (0): Linear(in_features=1536, out_features=6144, bias=True)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=6144, out_features=1536, bias=True)\n  )\n  (ff_dropout): Dropout(p=0.2, inplace=False)\n  (ff_layer_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n)",
    "dropout": "unknown",
    "stats": {
        "iter_through": 1550,
        "tok_through": 25395200,
        "losses": [
            10.625,
            9.9375,
            9.875,
            9.8125,
            9.4375,
            9.4375,
            9.3125,
            9.125,
            9.1875,
            9.0,
            9.0,
            9.0,
            9.0625,
            8.875,
            8.875,
            8.625,
            9.0,
            8.4375,
            8.625,
            8.4375,
            8.5,
            8.3125,
            8.1875,
            8.3125,
            8.25,
            8.375,
            8.4375,
            7.90625,
            8.25,
            8.25,
            8.0625
        ],
        "tok_snap": 3358720,
        "time_snap": 1750920159.2076998,
        "run_time_start": 1750919920.2089062,
        "run_tok_through": 25395200,
        "run_iter_through": 1550
    }
}